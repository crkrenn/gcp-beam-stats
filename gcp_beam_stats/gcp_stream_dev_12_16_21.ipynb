{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9afaeebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interactive!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: keep working on pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d482fb3d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3793255570.py, line 135)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/ml/68vtzhgn0pz0bddnq90khpq80000gp/T/ipykernel_80436/3793255570.py\"\u001b[0;36m, line \u001b[0;32m135\u001b[0m\n\u001b[0;31m    m        ic(len(df_merge))\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# dev pruning\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "stream data to a Pub/Sub topic\n",
    "\"\"\"\n",
    "\n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')\n",
    "\n",
    "\n",
    "# merge seconds -> minutes\n",
    "# update seconds -> minutes\n",
    "# partial aggregation\n",
    "\n",
    "# logarithmic publishing (every 12 seconds; 5 hours, 6 days, etc.)\n",
    "# original publish at finest scale\n",
    "# select, bin, publish, delete\n",
    "# add old data if appropriate\n",
    "# when? every agregation_label[1]\n",
    "# verify that we are not agregating twice. \n",
    "# 10 trips in each bin\n",
    "\n",
    "# print(\"adding pruned_test data\")\n",
    "# aggregation_lengths = [120, 120, 48, 60, 24, 10]\n",
    "# aggregation_labels = (\n",
    "#     [\"seconds\", \"minutes\", \"hours\", \"days\", \"months\", \"years\"])\n",
    "# date0 = datetime(2021, 10, 31)\n",
    "\n",
    "# min/max object lists not updating correctly\n",
    "\n",
    "# dev packages:\n",
    "import sys\n",
    "\n",
    "# prod packages\n",
    "import os\n",
    "import distogram\n",
    "from datetime import datetime\n",
    "import jsonpickle\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "try:\n",
    "    from icecream import ic\n",
    "except ImportError:  # Graceful fallback if IceCream isn't installed.\n",
    "    ic = lambda *a: None if not a else (a[0] if len(a) == 1 else a)  # noqa\n",
    "\n",
    "\n",
    "from common import (\n",
    "    Base, return_test_engine, LabelledDistogram, make_distribution, \n",
    "    make_distogram, AggregationType, delete_tables, taxi_data_headers_map)\n",
    "\n",
    "debug = False\n",
    "\n",
    "try:\n",
    "    import zoneinfo\n",
    "except ImportError:\n",
    "    from backports import zoneinfo\n",
    "\n",
    "    \n",
    "    \n",
    "project_id = os.environ.get('DEVSHELL_PROJECT_ID')\n",
    "dataset = 'default_dataset'\n",
    "sleep = 1.0  # seconds\n",
    "batch_size = 10000\n",
    "\n",
    "# store histograms, time, table, project, \n",
    "\n",
    "\n",
    "def prune_test_data(engine):\n",
    "    Base.metadata.create_all(engine)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    aggregation_lengths = [120, 120, 48, 60, 24, 10]\n",
    "    aggregation_labels = (\n",
    "        [\"seconds\", \"minutes\", \"hours\", \"days\", \"months\", \"years\"])\n",
    "\n",
    "    # load initial data\n",
    "    labelled_distogram_list = list(\n",
    "        session.query(LabelledDistogram).order_by(\n",
    "            LabelledDistogram.datetime)) \n",
    "    labelled_distogram_hash = {\n",
    "        instance.primary_key: instance for instance in labelled_distogram_list}\n",
    "\n",
    "    df_metadata = pd.DataFrame(\n",
    "        [instance.metadata_list for instance in labelled_distogram_list],\n",
    "        columns=labelled_distogram_list[0].metadata_labels)\n",
    "\n",
    "    ic(df_metadata.columns)\n",
    "    if is_interactive():\n",
    "#         display(df_metadata)\n",
    "        pass\n",
    "    first_time = df_metadata.iloc[0].at['datetime']\n",
    "    last_time = df_metadata.iloc[-1].at['datetime']\n",
    "    df_metadata['parsed'] = False\n",
    "    \n",
    "    # combine seconds to minutes\n",
    "    # @TODO: writing time rounding function\n",
    "    aggregation_index = 0\n",
    "    df_fine = df_metadata[\n",
    "        df_metadata['aggregation_type'] == aggregation_labels[aggregation_index]]\n",
    "    \n",
    "    delta_dict = {\n",
    "        aggregation_labels[aggregation_index]: aggregation_lengths[aggregation_index]}\n",
    "    min_keep_time = last_time - relativedelta(**delta_dict)\n",
    "\n",
    "    delta_dict = {aggregation_labels[aggregation_index + 1]: 1}\n",
    "\n",
    "    merge_stop_time = last_time + relativedelta(seconds = 1)\n",
    "#     while merge_stop_time > first_time:\n",
    "#     if True:\n",
    "#     ic(len(df_fine))\n",
    "    for i in range(10):\n",
    "        merge_start_time = merge_stop_time - relativedelta(**delta_dict)\n",
    "\n",
    "        rounding_dict = {\"microsecond\": 0}\n",
    "        for j in range(aggregation_index + 1):\n",
    "            rounding_dict[aggregation_labels[j][:-1]] = 0\n",
    "        merge_start_time = merge_start_time.replace(**rounding_dict)\n",
    "#         ic(i)\n",
    "#         ic(merge_start_time)\n",
    "#         ic(merge_stop_time)\n",
    "        \n",
    "        df_mask = ((df_fine[\"datetime\"] < merge_stop_time)\n",
    "                    & (df_fine[\"datetime_min\"] >= merge_start_time))\n",
    "        df_merge = df_fine[df_mask]\n",
    "        df_fine.loc[df_mask, \"parsed\"] = True\n",
    "m        ic(len(df_merge))\n",
    "        ic(df_merge)\n",
    "        if is_interactive():\n",
    "            pass\n",
    "#             display(df_merge)\n",
    "        if len(df_mask) > 0:\n",
    "            merge_stop_time = merge_start_time\n",
    "#             ic()\n",
    "        else:\n",
    "            merge_stop_time = df_fine[df_fine[\"parsed\"] == False].iloc[-1].at['datetime']\n",
    "            merge_stop_time = merge_stop_time + relativedelta(seconds = 1)\n",
    "            ic()\n",
    "            \n",
    "#     df_merge = df_metadata[(\n",
    "#         df_metadata['datetime']]\n",
    "\n",
    "# df1 = df[(df.a != -1) & (df.b != -1)]\n",
    "\n",
    "def publish_pruned_test_data(engine):\n",
    "    Base.metadata.create_all(engine)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    if True:\n",
    "        print(\"adding pruned_test data\")\n",
    "        aggregation_lengths = [120, 120, 48, 60, 24, 10]\n",
    "        aggregation_labels = (\n",
    "            [\"seconds\", \"minutes\", \"hours\", \"days\", \"months\", \"years\"])\n",
    "        date0 = datetime(2021, 10, 31)\n",
    "        for i in range(len(AggregationType)):\n",
    "        # i = 0\n",
    "        # if True:\n",
    "            print(AggregationType(i).name)\n",
    "            j_min = 0\n",
    "            j_max = 5 * aggregation_lengths[i]\n",
    "            j_delta = aggregation_lengths[i] // 5\n",
    "            last_date = date0\n",
    "            for j in range(j_min, j_max, j_delta):\n",
    "                delta_dict = {aggregation_labels[i]: -j}\n",
    "                delta = relativedelta(**delta_dict)\n",
    "                date = date0 + delta\n",
    "                # America/New_York\n",
    "                date.replace(tzinfo=zoneinfo.ZoneInfo('Etc/UTC'))\n",
    "                h = make_distogram(make_distribution())\n",
    "                d = LabelledDistogram(\n",
    "                    data_source=\"debug3\",\n",
    "                    variable_name=\"x\",\n",
    "                    datetime_min=last_date,\n",
    "                    datetime=date,\n",
    "                    aggregation_type=AggregationType(0).name,\n",
    "                    temporary_record=False,\n",
    "                    distogram=h)\n",
    "                session.add(d)\n",
    "                session.commit()\n",
    "                last_date = date\n",
    "        print(\"before commit\")\n",
    "        session.commit()\n",
    "        print(\"after commit\")\n",
    "\n",
    "\n",
    "def publish_taxi_data(engine):\n",
    "\n",
    "    def publish_single_time():\n",
    "        for header, my_distogram in distogram_dict.items():\n",
    "            d = LabelledDistogram(\n",
    "                data_source=\"taxi_min_max\",\n",
    "                variable_name=header,\n",
    "                datetime=max_datetime,\n",
    "                aggregation_type=aggregation_type,\n",
    "                distogram=my_distogram)\n",
    "            session.add(d)\n",
    "            session.commit()\n",
    "        print(f\"published data: {max_datetime}\")\n",
    "        print(f\"data_source: {d.data_source}\")   \n",
    "             \n",
    "    Base.metadata.create_all(engine)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    print(\"taxi!\")\n",
    "    # minutes/hours/days/weeks\n",
    "    df = pd.read_parquet('data.parquet')\n",
    "    df.sort_values(by=['tpep_dropoff_datetime'])\n",
    "    # df2 = df.tail(1000000)['dropoff_latitude']\n",
    "    # df2.to_csv('latitude.csv')\n",
    "    # sys.exit()\n",
    "    aggregation_dict = {\n",
    "        # \"minutes\": 15, \n",
    "        # \"hours\": 1, \n",
    "        \"days\": 1}\n",
    "    for aggregation_type, aggregation_length in aggregation_dict.items():\n",
    "\n",
    "        delta_time_dict = {aggregation_type: aggregation_length}\n",
    "        delta_time = relativedelta(**delta_time_dict)\n",
    "        dt0 = df.iloc[0]['tpep_dropoff_datetime']\n",
    "        print(f\"dt0: {dt0}\")\n",
    "        if aggregation_type in {\"minutes\", \"hours\"}:\n",
    "            dt0 = datetime(*dt0.timetuple()[:4])  # year/month/day/hour\n",
    "        else:\n",
    "            dt0 = datetime(*dt0.timetuple()[:3])  # year/month/day\n",
    "        print(f\"dt0: {dt0}\")\n",
    "        min_datetime = dt0\n",
    "        max_datetime = dt0 + delta_time\n",
    "        create_new_distograms = True\n",
    "        for index, row in df.tail(1000000).iterrows():\n",
    "            if create_new_distograms:\n",
    "                distogram_dict = {\n",
    "                    header: distogram.Distogram(with_min_max_list=True) \n",
    "                    for header in taxi_data_headers_map\n",
    "                    if header != 'tpep_dropoff_datetime'}\n",
    "                create_new_distograms = False\n",
    "            for header, my_distogram in distogram_dict.items():\n",
    "                if debug:\n",
    "                    print(f\"json pickle\")\n",
    "                    # print(f\"empty: {jsonpickle.encode(my_distogram, indent=2)}\")\n",
    "                    my_distogram = distogram.update(my_distogram, row[header], obj=row.to_dict())\n",
    "                    # print(f\"None max_min: {jsonpickle.encode(my_distogram, indent=2)}\")\n",
    "                    my_distogram = distogram.update(my_distogram, row[header], obj=row.to_dict())\n",
    "                    print(f\"Row max_min: {jsonpickle.encode(my_distogram, indent=2)}\")\n",
    "                    # print(f\"Row.to_dict: {jsonpickle.encode(row.to_dict(), indent=2)}\")\n",
    "                    sys.exit()\n",
    "                # my_distogram = distogram.update(my_distogram, row[header], obj=row.to_dict())\n",
    "                my_distogram = distogram.update(my_distogram, row[header], obj=row.to_dict())\n",
    "            if row['tpep_dropoff_datetime'] > max_datetime:\n",
    "                create_new_distograms = True \n",
    "                publish_single_time()\n",
    "                min_datetime = max_datetime\n",
    "                max_datetime = max_datetime + delta_time\n",
    "        publish_single_time()\n",
    "\n",
    "\n",
    "def main(engine):\n",
    "\n",
    "    print(\"before create\")\n",
    "    Base.metadata.create_all(engine)\n",
    "\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    print(\"before read\")\n",
    "    success = False\n",
    "    for instance in (\n",
    "        session.query(LabelledDistogram).order_by(\n",
    "            LabelledDistogram.primary_key)):\n",
    "        success = True\n",
    "        # print(instance.primary_key, instance.variable_name, instance.datetime)\n",
    "\n",
    "\n",
    "    if success:\n",
    "        print()\n",
    "        print(instance.primary_key, instance.variable_name)\n",
    "        h2 = jsonpickle.decode(instance.distogram_string)\n",
    "        print(f\"min/mean/max {h2.min}/{distogram.mean(h2)}/{h2.max}\")\n",
    "\n",
    "\n",
    "\n",
    "    # publish_taxi_data(engine)\n",
    "    # every \n",
    "    print(\"read from database\")\n",
    "    for instance in (\n",
    "        session.query(LabelledDistogram).order_by(\n",
    "            LabelledDistogram.primary_key)):\n",
    "        pass\n",
    "    print(\n",
    "        instance.primary_key,\n",
    "        instance.variable_name,\n",
    "        instance.aggregation_type,\n",
    "        instance.data_source,\n",
    "        instance.datetime)\n",
    "    \n",
    "        # publish_pruned_test_data(engine)\n",
    "#     prune_test_data(engine)\n",
    "\n",
    "# from datetime import date\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# six_months = date.today() + relativedelta(months=+6)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    database_list = [\n",
    "        \"bigquery\", \"sqlite-memory\", \"sqlite-disk\", \"sqlite-disk-2\", \"postgres\"]\n",
    "    database = database_list[3]\n",
    "    engine = return_test_engine(database)\n",
    "    print(engine)\n",
    "\n",
    "    # test(engine)\n",
    "#     main(engine)\n",
    "#     os.remove(\"./localdb-2\")\n",
    "#     publish_pruned_test_data(engine)\n",
    "\n",
    "    prune_test_data(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37e97a5",
   "metadata": {
    "code_folding": [
     126,
     167,
     237
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine(sqlite:///./localdb-2)\n",
      "2021-12-19 06:45:23,215 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2021-12-19 06:45:23,216 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"distograms\")\n",
      "2021-12-19 06:45:23,218 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2021-12-19 06:45:23,219 INFO sqlalchemy.engine.Engine COMMIT\n",
      "2021-12-19 06:45:23,221 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2021-12-19 06:45:23,223 INFO sqlalchemy.engine.Engine SELECT distograms.primary_key AS distograms_primary_key, distograms.data_source AS distograms_data_source, distograms.variable_name AS distograms_variable_name, distograms.datetime AS distograms_datetime, distograms.datetime_min AS distograms_datetime_min, distograms.distogram_string AS distograms_distogram_string, distograms.aggregation_type AS distograms_aggregation_type, distograms.temporary_record AS distograms_temporary_record \n",
      "FROM distograms ORDER BY distograms.datetime\n",
      "2021-12-19 06:45:23,225 INFO sqlalchemy.engine.Engine [generated in 0.00123s] ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| df_metadata.columns: Index(['primary_key', 'data_source', 'variable_name', 'datetime',\n",
      "                                'datetime_min', 'aggregation_type', 'temporary_record', 'mean', 'stdev',\n",
      "                                'max', 'min'],\n",
      "                               dtype='object')\n",
      "ic| len(df_merge): 14\n",
      "ic| len(df_merge): 9\n",
      "ic| len(df_merge): 8\n",
      "ic| len(df_merge): 9\n",
      "ic| len(df_merge): 8\n",
      "ic| len(df_merge): 9\n",
      "ic| len(df_merge): 8\n",
      "ic| len(df_merge): 9\n",
      "ic| len(df_merge): 8\n",
      "ic| len(df_merge): 7\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "stream data to a Pub/Sub topic\n",
    "\"\"\"\n",
    "\n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')\n",
    "\n",
    "\n",
    "# merge seconds -> minutes\n",
    "# update seconds -> minutes\n",
    "# partial aggregation\n",
    "\n",
    "# logarithmic publishing (every 12 seconds; 5 hours, 6 days, etc.)\n",
    "# original publish at finest scale\n",
    "# select, bin, publish, delete\n",
    "# add old data if appropriate\n",
    "# when? every agregation_label[1]\n",
    "# verify that we are not agregating twice. \n",
    "# 10 trips in each bin\n",
    "\n",
    "# print(\"adding pruned_test data\")\n",
    "# aggregation_lengths = [120, 120, 48, 60, 24, 10]\n",
    "# aggregation_labels = (\n",
    "#     [\"seconds\", \"minutes\", \"hours\", \"days\", \"months\", \"years\"])\n",
    "# date0 = datetime(2021, 10, 31)\n",
    "\n",
    "# min/max object lists not updating correctly\n",
    "\n",
    "# dev packages:\n",
    "import sys\n",
    "\n",
    "# prod packages\n",
    "import os\n",
    "import distogram\n",
    "from datetime import datetime\n",
    "import jsonpickle\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "try:\n",
    "    from icecream import ic\n",
    "except ImportError:  # Graceful fallback if IceCream isn't installed.\n",
    "    ic = lambda *a: None if not a else (a[0] if len(a) == 1 else a)  # noqa\n",
    "\n",
    "\n",
    "from common import (\n",
    "    Base, return_test_engine, LabelledDistogram, make_distribution, \n",
    "    make_distogram, AggregationType, delete_tables, taxi_data_headers_map)\n",
    "\n",
    "debug = False\n",
    "\n",
    "try:\n",
    "    import zoneinfo\n",
    "except ImportError:\n",
    "    from backports import zoneinfo\n",
    "\n",
    "    \n",
    "    \n",
    "project_id = os.environ.get('DEVSHELL_PROJECT_ID')\n",
    "dataset = 'default_dataset'\n",
    "sleep = 1.0  # seconds\n",
    "batch_size = 10000\n",
    "\n",
    "# store histograms, time, table, project, \n",
    "\n",
    "\n",
    "def prune_test_data(engine):\n",
    "    Base.metadata.create_all(engine)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    aggregation_lengths = [120, 120, 48, 60, 24, 10]\n",
    "    aggregation_labels = (\n",
    "        [\"seconds\", \"minutes\", \"hours\", \"days\", \"months\", \"years\"])\n",
    "\n",
    "    # load initial data\n",
    "    labelled_distogram_list = list(\n",
    "        session.query(LabelledDistogram).order_by(\n",
    "            LabelledDistogram.datetime)) \n",
    "    labelled_distogram_hash = {\n",
    "        instance.primary_key: instance for instance in labelled_distogram_list}\n",
    "\n",
    "    df_metadata = pd.DataFrame(\n",
    "        [instance.metadata_list for instance in labelled_distogram_list],\n",
    "        columns=labelled_distogram_list[0].metadata_labels)\n",
    "\n",
    "    ic(df_metadata.columns)\n",
    "    if is_interactive():\n",
    "#         display(df_metadata)\n",
    "        pass\n",
    "    first_time = df_metadata.iloc[0].at['datetime']\n",
    "    last_time = df_metadata.iloc[-1].at['datetime']\n",
    "    df_metadata['parsed'] = False\n",
    "    \n",
    "    # combine seconds to minutes\n",
    "    # @TODO: writing time rounding function\n",
    "    aggregation_index = 0\n",
    "    df_fine = df_metadata[\n",
    "        df_metadata['aggregation_type'] == aggregation_labels[aggregation_index]]\n",
    "    \n",
    "    delta_dict = {\n",
    "        aggregation_labels[aggregation_index]: aggregation_lengths[aggregation_index]}\n",
    "    min_keep_time = last_time - relativedelta(**delta_dict)\n",
    "\n",
    "    delta_dict = {aggregation_labels[aggregation_index + 1]: 1}\n",
    "\n",
    "    merge_stop_time = last_time + relativedelta(seconds = 1)\n",
    "#     while merge_stop_time > first_time:\n",
    "#     if True:\n",
    "#     ic(len(df_fine))\n",
    "    for i in range(10):\n",
    "        merge_start_time = merge_stop_time - relativedelta(**delta_dict)\n",
    "\n",
    "        rounding_dict = {\"microsecond\": 0}\n",
    "        for j in range(aggregation_index + 1):\n",
    "            rounding_dict[aggregation_labels[j][:-1]] = 0\n",
    "        merge_start_time = merge_start_time.replace(**rounding_dict)\n",
    "#         ic(i)\n",
    "#         ic(merge_start_time)\n",
    "#         ic(merge_stop_time)\n",
    "        \n",
    "        df_mask = ((df_fine[\"datetime\"] < merge_stop_time)\n",
    "                    & (df_fine[\"datetime_min\"] >= merge_start_time))\n",
    "        df_merge = df_fine[df_mask]\n",
    "        df_fine.loc[df_mask, \"parsed\"] = True\n",
    "        ic(len(df_merge))\n",
    "\n",
    "        if is_interactive():\n",
    "            pass\n",
    "#             display(df_merge)\n",
    "        if len(df_mask) > 0:\n",
    "            merge_stop_time = merge_start_time\n",
    "#             ic()\n",
    "        else:\n",
    "            merge_stop_time = df_fine[df_fine[\"parsed\"] == False].iloc[-1].at['datetime']\n",
    "            merge_stop_time = merge_stop_time + relativedelta(seconds = 1)\n",
    "            ic()\n",
    "            \n",
    "#     df_merge = df_metadata[(\n",
    "#         df_metadata['datetime']]\n",
    "\n",
    "# df1 = df[(df.a != -1) & (df.b != -1)]\n",
    "\n",
    "def publish_pruned_test_data(engine):\n",
    "    Base.metadata.create_all(engine)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    if True:\n",
    "        print(\"adding pruned_test data\")\n",
    "        aggregation_lengths = [120, 120, 48, 60, 24, 10]\n",
    "        aggregation_labels = (\n",
    "            [\"seconds\", \"minutes\", \"hours\", \"days\", \"months\", \"years\"])\n",
    "        date0 = datetime(2021, 10, 31)\n",
    "        for i in range(len(AggregationType)):\n",
    "        # i = 0\n",
    "        # if True:\n",
    "            print(AggregationType(i).name)\n",
    "            j_min = 0\n",
    "            j_max = 5 * aggregation_lengths[i]\n",
    "            j_delta = aggregation_lengths[i] // 5\n",
    "            last_date = date0\n",
    "            for j in range(j_min, j_max, j_delta):\n",
    "                delta_dict = {aggregation_labels[i]: -j}\n",
    "                delta = relativedelta(**delta_dict)\n",
    "                date = date0 + delta\n",
    "                # America/New_York\n",
    "                date.replace(tzinfo=zoneinfo.ZoneInfo('Etc/UTC'))\n",
    "                h = make_distogram(make_distribution())\n",
    "                d = LabelledDistogram(\n",
    "                    data_source=\"debug3\",\n",
    "                    variable_name=\"x\",\n",
    "                    datetime_min=last_date,\n",
    "                    datetime=date,\n",
    "                    aggregation_type=AggregationType(0).name,\n",
    "                    temporary_record=False,\n",
    "                    distogram=h)\n",
    "                session.add(d)\n",
    "                session.commit()\n",
    "                last_date = date\n",
    "        print(\"before commit\")\n",
    "        session.commit()\n",
    "        print(\"after commit\")\n",
    "\n",
    "\n",
    "def publish_taxi_data(engine):\n",
    "\n",
    "    def publish_single_time():\n",
    "        for header, my_distogram in distogram_dict.items():\n",
    "            d = LabelledDistogram(\n",
    "                data_source=\"taxi_min_max\",\n",
    "                variable_name=header,\n",
    "                datetime=max_datetime,\n",
    "                aggregation_type=aggregation_type,\n",
    "                distogram=my_distogram)\n",
    "            session.add(d)\n",
    "            session.commit()\n",
    "        print(f\"published data: {max_datetime}\")\n",
    "        print(f\"data_source: {d.data_source}\")   \n",
    "             \n",
    "    Base.metadata.create_all(engine)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    print(\"taxi!\")\n",
    "    # minutes/hours/days/weeks\n",
    "    df = pd.read_parquet('data.parquet')\n",
    "    df.sort_values(by=['tpep_dropoff_datetime'])\n",
    "    # df2 = df.tail(1000000)['dropoff_latitude']\n",
    "    # df2.to_csv('latitude.csv')\n",
    "    # sys.exit()\n",
    "    aggregation_dict = {\n",
    "        # \"minutes\": 15, \n",
    "        # \"hours\": 1, \n",
    "        \"days\": 1}\n",
    "    for aggregation_type, aggregation_length in aggregation_dict.items():\n",
    "\n",
    "        delta_time_dict = {aggregation_type: aggregation_length}\n",
    "        delta_time = relativedelta(**delta_time_dict)\n",
    "        dt0 = df.iloc[0]['tpep_dropoff_datetime']\n",
    "        print(f\"dt0: {dt0}\")\n",
    "        if aggregation_type in {\"minutes\", \"hours\"}:\n",
    "            dt0 = datetime(*dt0.timetuple()[:4])  # year/month/day/hour\n",
    "        else:\n",
    "            dt0 = datetime(*dt0.timetuple()[:3])  # year/month/day\n",
    "        print(f\"dt0: {dt0}\")\n",
    "        min_datetime = dt0\n",
    "        max_datetime = dt0 + delta_time\n",
    "        create_new_distograms = True\n",
    "        for index, row in df.tail(1000000).iterrows():\n",
    "            if create_new_distograms:\n",
    "                distogram_dict = {\n",
    "                    header: distogram.Distogram(with_min_max_list=True) \n",
    "                    for header in taxi_data_headers_map\n",
    "                    if header != 'tpep_dropoff_datetime'}\n",
    "                create_new_distograms = False\n",
    "            for header, my_distogram in distogram_dict.items():\n",
    "                if debug:\n",
    "                    print(f\"json pickle\")\n",
    "                    # print(f\"empty: {jsonpickle.encode(my_distogram, indent=2)}\")\n",
    "                    my_distogram = distogram.update(my_distogram, row[header], obj=row.to_dict())\n",
    "                    # print(f\"None max_min: {jsonpickle.encode(my_distogram, indent=2)}\")\n",
    "                    my_distogram = distogram.update(my_distogram, row[header], obj=row.to_dict())\n",
    "                    print(f\"Row max_min: {jsonpickle.encode(my_distogram, indent=2)}\")\n",
    "                    # print(f\"Row.to_dict: {jsonpickle.encode(row.to_dict(), indent=2)}\")\n",
    "                    sys.exit()\n",
    "                # my_distogram = distogram.update(my_distogram, row[header], obj=row.to_dict())\n",
    "                my_distogram = distogram.update(my_distogram, row[header], obj=row.to_dict())\n",
    "            if row['tpep_dropoff_datetime'] > max_datetime:\n",
    "                create_new_distograms = True \n",
    "                publish_single_time()\n",
    "                min_datetime = max_datetime\n",
    "                max_datetime = max_datetime + delta_time\n",
    "        publish_single_time()\n",
    "\n",
    "\n",
    "def main(engine):\n",
    "\n",
    "    print(\"before create\")\n",
    "    Base.metadata.create_all(engine)\n",
    "\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    print(\"before read\")\n",
    "    success = False\n",
    "    for instance in (\n",
    "        session.query(LabelledDistogram).order_by(\n",
    "            LabelledDistogram.primary_key)):\n",
    "        success = True\n",
    "        # print(instance.primary_key, instance.variable_name, instance.datetime)\n",
    "\n",
    "\n",
    "    if success:\n",
    "        print()\n",
    "        print(instance.primary_key, instance.variable_name)\n",
    "        h2 = jsonpickle.decode(instance.distogram_string)\n",
    "        print(f\"min/mean/max {h2.min}/{distogram.mean(h2)}/{h2.max}\")\n",
    "\n",
    "\n",
    "\n",
    "    # publish_taxi_data(engine)\n",
    "    # every \n",
    "    print(\"read from database\")\n",
    "    for instance in (\n",
    "        session.query(LabelledDistogram).order_by(\n",
    "            LabelledDistogram.primary_key)):\n",
    "        pass\n",
    "    print(\n",
    "        instance.primary_key,\n",
    "        instance.variable_name,\n",
    "        instance.aggregation_type,\n",
    "        instance.data_source,\n",
    "        instance.datetime)\n",
    "    \n",
    "        # publish_pruned_test_data(engine)\n",
    "#     prune_test_data(engine)\n",
    "\n",
    "# from datetime import date\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# six_months = date.today() + relativedelta(months=+6)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    database_list = [\n",
    "        \"bigquery\", \"sqlite-memory\", \"sqlite-disk\", \"sqlite-disk-2\", \"postgres\"]\n",
    "    database = database_list[3]\n",
    "    engine = return_test_engine(database)\n",
    "    print(engine)\n",
    "\n",
    "    # test(engine)\n",
    "#     main(engine)\n",
    "#     os.remove(\"./localdb-2\")\n",
    "#     publish_pruned_test_data(engine)\n",
    "\n",
    "    prune_test_data(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697562a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#gcp_app.py\n",
    "#######################\n",
    "# notes\n",
    "#######################\n",
    "\n",
    "# Future development/polishing:\n",
    "# track representative points for each histogram bin as well as min/max\n",
    "\n",
    "# use uuid again?\n",
    "# object dict\n",
    "\n",
    "# consider logarithmic time axis: \n",
    "# https://community.plotly.com/t/plotting-time-dates-on-a-log-axis/33205/7\n",
    "\n",
    "# time series from postgres; histogram from time series point\n",
    "# see \"shiny_example_2\"\n",
    "\n",
    "# MVP: time series/histogram/extrema/integration/table\n",
    "# 12: extrema 1; 6/1/4/1\n",
    "\n",
    "# dash plan:\n",
    "# row 0 controls\n",
    "# row 1 min/histogram/max\n",
    "# row 2 time series/table\n",
    "\n",
    "# consider row 1 time series/histogram\n",
    "\n",
    "# controls: integrated/discrete; variable\n",
    "\n",
    "# functions: select data\n",
    "\n",
    "# dtructure\n",
    "\n",
    "#######################\n",
    "# python imports\n",
    "#######################\n",
    "\n",
    "# system\n",
    "import json\n",
    "import math\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# community\n",
    "import distogram\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# plotly/dash\n",
    "import dash\n",
    "import plotly.express as px\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.graph_objs as go\n",
    "from dash import dash_table as dt\n",
    "\n",
    "# local\n",
    "from common import (\n",
    "    LabelledDistogram, return_test_engine, histogram_step_plot_data)\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')\n",
    "\n",
    "if is_interactive():\n",
    "    print(\"interactive!\")\n",
    "    from jupyter_dash import JupyterDash\n",
    "    \n",
    "# # conditional imports\n",
    "# try:\n",
    "#     import zoneinfo\n",
    "# except ImportError:\n",
    "#     from backports import zoneinfo\n",
    "\n",
    "#######################\n",
    "# Debug\n",
    "#######################\n",
    "\n",
    "\n",
    "#######################\n",
    "# Data Analysis / Model\n",
    "#######################\n",
    "\n",
    "# choose database (currently postgres)\n",
    "load_dotenv()\n",
    "database_list = [\n",
    "    \"bigquery\", \"sqlite-memory\", \"sqlite-disk\", \"postgres\"]\n",
    "database = database_list[2]\n",
    "engine = return_test_engine(database)\n",
    "\n",
    "# load initial data\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "labelled_distogram_list = list(\n",
    "    session.query(LabelledDistogram).order_by(\n",
    "        LabelledDistogram.datetime)) \n",
    "labelled_distogram_hash = {\n",
    "    instance.primary_key: instance for instance in labelled_distogram_list}\n",
    "\n",
    "df_metadata = pd.DataFrame(\n",
    "    [instance.metadata_list for instance in labelled_distogram_list],\n",
    "    columns=labelled_distogram_list[0].metadata_labels)\n",
    "\n",
    "\n",
    "#########################\n",
    "# Dashboard Layout / View\n",
    "#########################\n",
    "\n",
    "def gcp_control(label, choices):\n",
    "    return dbc.Card(\n",
    "        [html.Div(\n",
    "            [dbc.Label(label.replace(\"_\", \" \")),\n",
    "             dcc.Dropdown(\n",
    "                id=label,\n",
    "                options=[\n",
    "                    {\"label\": item, \"value\": item} for item in choices],\n",
    "                value=choices[0],\n",
    "                searchable=False,\n",
    "            )])\n",
    "         ]\n",
    "    )\n",
    "\n",
    "if is_interactive():\n",
    "    app = JupyterDash(external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "else:\n",
    "    app = dash.Dash(external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "app.layout = dbc.Container(\n",
    "    [\n",
    "        html.H1('{Streaming} summary statistics'),\n",
    "        html.Hr(),\n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col(\n",
    "                    gcp_control(\n",
    "                        'data_source', \n",
    "                        df_metadata['data_source'].unique()\n",
    "                    ), md=3),\n",
    "                dbc.Col(\n",
    "                    gcp_control(\n",
    "                        'variable_name', \n",
    "                        df_metadata['variable_name'].unique()\n",
    "                    ), md=3),\n",
    "\n",
    "                dbc.Col(\n",
    "                    gcp_control(\n",
    "                        'aggregation_type', \n",
    "                        df_metadata['aggregation_type'].unique()\n",
    "                    ), md=3),\n",
    "            ],\n",
    "            align='center',\n",
    "        ),        \n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col(dcc.Graph(id='time-series-graph'), md=5),\n",
    "                dbc.Col([dbc.Col(id='min-table')], md=1),\n",
    "                dbc.Col(dcc.Graph(id='histogram-graph'), md=5),\n",
    "                dbc.Col([dbc.Col(id='max-table')], md=1),\n",
    "            ],\n",
    "            align='center',\n",
    "        ),\n",
    "        # dbc.Row(\n",
    "        #     [\n",
    "        #         dbc.Col([dbc.Col(id='detail-table')], md=12),\n",
    "        #     ],\n",
    "        #     align='center',\n",
    "        # ),\n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col(html.Pre(id='click-data'), md=4),\n",
    "                dbc.Col(html.Pre(id='click-data-2'), md=4),\n",
    "            ],\n",
    "            align=\"center\",\n",
    "        ),\n",
    "    ],\n",
    "    fluid=True,\n",
    ")\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Interaction Between Components / Controller\n",
    "#############################################\n",
    "\n",
    "def get_instance(data_source, variable_name, aggregation_type, clickData):\n",
    "    ctx = dash.callback_context\n",
    "    trigger = ctx.triggered[0]['prop_id'].split('.')[0]\n",
    "\n",
    "    if clickData == None or trigger != 'time-series-graph':\n",
    "        df = df_metadata\n",
    "        df_subset = df.loc[\n",
    "            (df['data_source'] == data_source) \n",
    "            & (df['variable_name'] == variable_name)\n",
    "            & (df['aggregation_type'] == aggregation_type)]\n",
    "        instance = labelled_distogram_hash[df_subset.iloc[-1].primary_key]\n",
    "    else:\n",
    "        instance = labelled_distogram_hash[\n",
    "            clickData['points'][0]['customdata']]\n",
    "    return instance\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('time-series-graph', 'figure'),\n",
    "    [\n",
    "        Input('data_source', 'value'),\n",
    "        Input('variable_name', 'value'),\n",
    "        Input('aggregation_type', 'value'),\n",
    "    ],\n",
    ")\n",
    "def make_time_series_graph(data_source, variable_name, aggregation_type):\n",
    "    df = df_metadata\n",
    "    df_subset = df.loc[\n",
    "        (df['data_source'] == data_source) \n",
    "        & (df['variable_name'] == variable_name)\n",
    "        & (df['aggregation_type'] == aggregation_type)]\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    x_label = 'datetime'\n",
    "    variable_list = ['max', 'mean', 'min']\n",
    "    # Loop df columns and plot columns to the figure\n",
    "    for variable in variable_list:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_subset[x_label], \n",
    "            y=df_subset[variable],\n",
    "            customdata=df_subset['primary_key'],\n",
    "            mode='markers',  # 'lines' or 'markers'\n",
    "            name=variable))\n",
    "    fig.update_layout(clickmode='event+select')\n",
    "    fig.update_xaxes(title_text=\"date/time\")\n",
    "    fig.update_yaxes(title_text=variable_name)\n",
    "    return fig\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('histogram-graph', 'figure'),\n",
    "    [\n",
    "        Input('data_source', 'value'),\n",
    "        Input('variable_name', 'value'),\n",
    "        Input('aggregation_type', 'value'),\n",
    "        Input('time-series-graph', 'clickData')\n",
    "    ],\n",
    ")\n",
    "def make_histogram(data_source, variable_name, aggregation_type, clickData):\n",
    "    instance = get_instance(\n",
    "        data_source, variable_name, aggregation_type, clickData)\n",
    "    hist = distogram.frequency_density_distribution(instance.distogram)\n",
    "    if hist == None:\n",
    "        fig = px.scatter(\n",
    "            x=[instance.distogram.bins[0][0]],\n",
    "            y=[1],\n",
    "            labels={\"x\": variable_name, \"y\": \"count\"}\n",
    "            )\n",
    "        return fig\n",
    "    labels = [\"bin\", \"count/bin width\"]\n",
    "    df_hist = histogram_step_plot_data(hist, labels)\n",
    "    fig = px.line(\n",
    "        df_hist,\n",
    "        x=labels[0],\n",
    "        y=labels[1],\n",
    "        labels = {labels[0]: variable_name},\n",
    "        log_y=True,\n",
    "    )\n",
    "    fig.update_traces(mode=\"lines\", line_shape=\"hv\")\n",
    "    fig.update_layout(clickmode='event+select')\n",
    "    return fig\n",
    "\n",
    "def make_min_max_table(\n",
    "    data_source, variable_name, aggregation_type, clickData,\n",
    "    min_max_type):\n",
    "    if min_max_type not in [\"min\", \"max\"]:\n",
    "        raise ValueError(\n",
    "            f\"{min_max_type} is not supported. \"\n",
    "            f\"Only 'min' and 'max' are supported.\")\n",
    "    instance = get_instance(\n",
    "        data_source, variable_name, aggregation_type, clickData)\n",
    "    if min_max_type == \"min\":\n",
    "        min_max_data = distogram.min_list(instance.distogram)\n",
    "    elif min_max_type == \"max\":\n",
    "        min_max_data = distogram.min_list(instance.distogram)\n",
    "    # calculate necessary significant figures\n",
    "    if len(min_max_data) > 1:\n",
    "        min_data = min(min_max_data)\n",
    "        max_data = max(min_max_data)\n",
    "        range_list = [min_max_data[i] - min_max_data[i-1] for i in range(1,len(min_max_data))]\n",
    "        range_list.sort()\n",
    "        range_list = [i for i in range_list if i > 0]\n",
    "        max_abs_data = max([abs(min_data), abs(max_data)])\n",
    "        sig_figs_needed = -int(math.log10(range_list[0]/max_abs_data))\n",
    "        format_string = \"{\" + f\":.{sig_figs_needed}e\" + \"}\"\n",
    "    else:\n",
    "        format_string = \"{f:.2e}\"\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        [format_string.format(i) \n",
    "                for i in min_max_data],\n",
    "        columns = [min_max_type])\n",
    "    return [dt.DataTable(\n",
    "        columns=[{\"name\": i, \"id\": i} for i in df.columns],\n",
    "        data=df.to_dict('records'),\n",
    "        style_cell={'fontSize': 12}\n",
    "    ),\n",
    "    ]\n",
    "\n",
    "@app.callback(\n",
    "    Output('min-table', 'children'),\n",
    "    [\n",
    "        Input('data_source', 'value'),\n",
    "        Input('variable_name', 'value'),\n",
    "        Input('aggregation_type', 'value'),\n",
    "        Input('time-series-graph', 'clickData')\n",
    "    ],\n",
    ")\n",
    "def make_min_table(data_source, variable_name, aggregation_type, clickData):\n",
    "    return make_min_max_table(\n",
    "        data_source, variable_name, aggregation_type, clickData, min_max_type=\"min\")\n",
    "\n",
    "@app.callback(\n",
    "    Output('max-table', 'children'),\n",
    "    [\n",
    "        Input('data_source', 'value'),\n",
    "        Input('variable_name', 'value'),\n",
    "        Input('aggregation_type', 'value'),\n",
    "        Input('time-series-graph', 'clickData')\n",
    "    ],\n",
    ")\n",
    "def make_max_table(data_source, variable_name, aggregation_type, clickData):\n",
    "    return make_min_max_table(\n",
    "        data_source, variable_name, aggregation_type, clickData, min_max_type=\"max\")\n",
    "\n",
    "# @app.callback(\n",
    "#     Output('detail-table', 'children'),\n",
    "#     [\n",
    "#         Input('data_source', 'value'),\n",
    "#         Input('variable_name', 'value'),\n",
    "#         Input('aggregation_type', 'value'),\n",
    "#         Input('time-series-graph', 'clickData'),\n",
    "#         Input('histogram-graph', 'clickData')\n",
    "#     ],\n",
    "# )\n",
    "# def make_max_table(data_source, variable_name, aggregation_type, clickData):\n",
    "#     instance = get_instance(\n",
    "#         data_source, variable_name, aggregation_type, clickData)\n",
    "#     return [dt.DataTable(\n",
    "#         data=[{'max': \"{:.2e}\".format(instance.distogram.max)}],\n",
    "#         columns=[{'name': 'max', 'id': 'max'}],\n",
    "#         style_cell={'fontSize': 12}\n",
    "#     ),\n",
    "#     ]\n",
    "\n",
    "@app.callback(\n",
    "    Output('click-data', 'children'),\n",
    "    Input('time-series-graph', 'clickData'))\n",
    "def display_click_data(clickData):\n",
    "    return json.dumps(clickData, indent=2)\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('click-data-2', 'children'),\n",
    "    Input('histogram-graph', 'clickData'))\n",
    "def display_click_data(clickData):\n",
    "    return json.dumps(clickData, indent=2)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Interaction Between Components / Controller\n",
    "#############################################\n",
    "\n",
    "\n",
    "# start Flask server\n",
    "# if __name__ == '__main__':\n",
    "#     app.run_server(\n",
    "#         debug=True,\n",
    "#         host='0.0.0.0',\n",
    "#         port=8050\n",
    "#     )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if is_interactive():\n",
    "        app.run_server(mode='inline')\n",
    "    else:\n",
    "        app.run_server(debug=True, port=8891)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e76158",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "import dash\n",
    "import dash_table\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/solar.csv')\n",
    "\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "app.layout = dash_table.DataTable(\n",
    "    id='table',\n",
    "    columns=[{\"name\": i, \"id\": i} for i in df.columns],\n",
    "    data=df.to_dict('records'),\n",
    ")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a70756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gcp-beam-stats",
   "language": "python",
   "name": "venv_gcp-beam-stats"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
